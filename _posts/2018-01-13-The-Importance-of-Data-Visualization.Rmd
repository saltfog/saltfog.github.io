---
title: "The Importance of Data Visualization."
description: Before we perform any analysis and come up with any assumptions about
  the distributions of and relationships between variables in our datasets, it is
  always a good idea to visualize our data in order to understand their properties
  and identify appropriate analytics techniques. In this post, let's see the dramatic
  differences in conclutions that we can make based on (1) simple statistics only,
  and (2) data visualization.
keywords: data
layout: post
tags:
- R
- Data Science
- Machine Learning
comments: no
output: 
  html_document: 
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

*From Wikipedia, the free encyclopedia*

All four sets are identical when examined using simple summary statistics, but vary considerably when graphed
Anscombe's quartet comprises four datasets that have nearly identical simple descriptive statistics, yet appear very different when graphed. Each dataset consists of eleven (x,y) points. They were constructed in 1973 by the statistician Francis Anscombe to demonstrate both the importance of graphing data before analyzing it and the effect of outliers on statistical properties. He described the article as being intended to counter the impression among statisticians that "numerical calculations are exact, but graphs are rough
The Anscombe dataset, which is found in the base R datasets packege, is handy for showing the importance of data visualization in data analysis. It consists of four datasets and each dataset consists of eleven (x,y) points.

**The Four Data Sets**

```{r echo=TRUE}
library(ggplot2)
library(dplyr)
library(reshape2)
```

```{r echo=FALSE}
data(anscombe)
anscombe
```

Let's massage the data to make the data more managable for analysis and plotting.

Create four groups: A, B, C D.

```{r echo=TRUE}
A=select(anscombe, x=x1,y=y1)
B=select(anscombe, x=x2,y=y2)
C=select(anscombe, x=x3,y=y3)
D=select(anscombe, x=x4,y=y4)
```

Let's add a third column to help visualize the data.

```{r echo=FALSE}
A$group ='A'
B$group ='B'
C$group ='C'
D$group ='D'

head(A,4)  # showing sample data points from A
```

Now, let's merge the four datasets.

```{r echo=TRUE}
alldata=rbind(A,B,C,D)  # merging all the four data sets
alldata[c(1,13,23,43),]  # showing sample
```

Now let's Compare their summary statistics

```{r echo=TRUE}
summaryStats =alldata%>%group_by(group)%>%summarize("Mean X"=mean(x),
                                       "Sample Variance X"=var(x),
                                       "Mean Y"=round(mean(y),2),
                                       "Sample Variance Y"=round(var(y),1),
                                       'Correlation between X and Y '=round(cor(x,y),2)
                                      )

models = alldata %>% 
      group_by(group) %>%
      do(mod = lm(y ~ x, data = .)) %>%
      do(data.frame(var = names(coef(.$mod)),
                    coef = round(coef(.$mod),2),
                    group = .$group)) %>%
dcast(., group~var, value.var = "coef")



summaryStats_and_linear_fit = cbind(summaryStats, data_frame("Linear Regression" =
                                    paste0("Y = ",models$"(Intercept)"," + ",models$x,"X")))

summaryStats_and_linear_fit
```

If we look only at the simple summary statistics shown above, we would *conclude* that these four data sets are identical.

What if we plot the four data sets?

```{r}
ggplot(alldata, aes(x = x,y = y)) + geom_point(shape = 21, colour = "black", fill = "grey", size = 3, alpha = 0.8) +
    ggtitle("Anscombe's Data Sets")+ geom_smooth(method = "lm", se = FALSE, color='blue') + 
    facet_wrap(~group, scales="free")
```

**As we can see from the figures above, the datasets are very different from each other. The Anscombe's quartet is a good example that shows that we have to visualize the relatonships, distributuions and outliers of our data and we should not rely only on simple statistics.**